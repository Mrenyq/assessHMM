\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{float}
\usepackage{array}

\title{Empirical Assessment of HMM and HDP-HSMM Methods}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Albert Lam, Kabir Sawhney, and Joey Yoo \\
  \texttt{albertlam, ksawhney, joeyyoo@uchicago.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
We empirically compare the accuracy of the Baum-Welch and Viterbi algorithms with the outputs of a Hierarchical Dirichlet Process Hidden Semi-Markov Model (HDP-HSMM) to estimate hidden state sequences from time-ordered observations. Specifically, we simulate agent pathways in a variety of GridWorld settings that vary in grid size, placement of obstacles, and transition mechanism, including settings which violate the Markov assumption implicit to traditional Hidden Markov Models (HMMs). We found that (to be filled in)
\end{abstract}

\section{Background and Introduction}

Algorithms to estimate the most likely sequence of hidden states from observations in a HMM rely on the Markov assumption -- that future states are conditionally independent of past states given the present state. The most popular method for estimating these states involves evaluating conditional likelihoods using the forward-backward algorithm and then inferring the most likely sequence of hidden states using the Viterbi algorithm, which relies on this assumption. However, it has been suggested that the Markov assumption need not be strictly met for these techniques to generate reasonably accurate estimates of hidden state sequences. We sought to empirically test this proposition on the problem of estimating an agent's path in a GridWorld setting. To do so, we generate sequences of agent pathways and their corresponding observations under a variety of assumptions, including settings where we enforce the Markov assumption and others where we relax it.

\subsection{Hidden Markov Model (HMM)}
A HMM is a model consisting of two sequences:
\begin{itemize}
  \item A state sequence $z = \{z_1, z_2, \dots, z_T\}$ where $z_t$ is the state at timestep $t$ which takes a value from a state space $\mathcal{Z}$ that is assumed to be static and finite for $t = 1, 2, \dots, T$.
  \item An observation (or emission) sequence $x = \{x_1, x_2, \dots, x_T\}$ where $x_t$ is the observation at timestep $t$ from an observation space $\mathcal{X}$ that is assumed to be static and finite for $t = 1, 2, \dots, T$.
\end{itemize}

The relationship between these two sequences are embedded in the transition probabilities and the emission probabilities of the HMM. In particular, let $\pi_{i,j} = \mathbb{P}(z_{t+1} = j | z_{t} = i)$ denote the probability of an agent transitioning from state $i$ to state $j$, which we assume to be independent of $t$, and $\mathbb{P}(z_t = \zeta | x_t = \xi)$ denote the probability of observing $\zeta$ given state $\xi$. Interpreting these probabilities as parameters of the HMM gives way to a natural parametric Bayesian approach to estimating these probabilities that involves augmenting and then marginalizing the joint distributions of $x$ and $z$, solving for forward and backward recursions, and then computing the maximum likelihood estimates via the Baum Welch algorithm. The transition and emission probabilities can finally be used to compute most-likely trajectories using the Viterbi algorithm.

\subsection{Hidden Semi-Markov Model (HSMM)}
A HSMM extends a HMM to settings where the sequence of states are not strictly Markovian - instead, the probability of transitioning to a new state $z_{s+1}$ depends on the elapsed time in the current state $z_s$. Here, we introduce $s = 1, \dots, S$ to index state transitions that remain Markovian, and use $t= 1, \dots, T$ to index the associated emissions that are not necessarily Markovian. Then, let $D_s \in [1, \dots, T]$ denote the duration that an agent remains in state $x_s$ with explicit mass function $\mathbb{P}(D_s = d_s | z_s = i)$. Figure ~\ref{fig:hsmm} illustrates such an explicit-duration HSMM, where we define $z_s$ as super-states of sub-states $z'_{t}$ which each emit one observation $x_{t}$. It follows that the super-states are Markovian but sub-states are not.

\begin{figure}[H]
\centering
\includegraphics[scale=0.15]{images/hsmm2.png}
\caption{HSMM where super state transitions are Markovian, while emissions associated with each state follow some explicit duration distribution.}
\label{fig:hsmm}
\end{figure}

\subsection{Hierarchical Dirichlet Process HSMM (HDP-HSMM)}
We begin this section by reviewing the formulation of a hierarchical Dirichlet process HMM (HDP-HMM), which is a Bayesian non-parametric treatment of the classical HMM. A HDP-HMM$(\gamma, \alpha, H)$ model is a process governed by:
\begin{itemize}
  \item $\beta_k \coloneqq \beta_k'\cdot \prod_{i=1}^{k-1} (1-\beta_i')$ where $\beta_i' \overset{\text{i.i.d}}{\sim} Beta(1,\gamma)$
  \item $\pi_k \overset{\text{i.i.d}}{\sim} DP(H(\beta), \alpha)$ for $k = 1, \dots, \infty$ where $\beta = \{\beta_k\}_{k=1}^{\infty}$
  \item $z_t \sim \pi_{z_{t-1}}$
  \item $x_t \sim h(\theta_{z_t})$ where $\theta_{k} \sim H$ for $k = 1, \dots, \infty$
\end{itemize}
where $\gamma, \alpha > 0$ can be interpreted as concentration parameters and $H$ is the base distribution of the Dirichlet process. Note that $\{\beta_k\}_{k=1}^{\infty}$ is generated from a stick-breaking process with concentration parameter $\gamma$. These are then used in the generation of  transition distributions $\{\pi_k \}_{k=1}^{\infty}$ from a Dirichlet process with parameters $H$ and $\alpha$, where $H$ is defined on the $\{\beta_k\}_{k=1}^{\infty}$ partitions of the measurement set. For $t = 1, \dots, T$, each state $z_t$ is drawn from a respective $\pi_{z_{t-1}}$, and each observation is drawn from an observation distribution $h(\theta_{z_t})$ where $\theta_{k}$ is drawn from $H$. Observe that as $\alpha$ decreases, distributions $\{\pi_k\}_{s=1}^{\infty}$ tend to be more concentrated, and as $\gamma$ decreases, $\{\beta_k\}_{k=1}^{\infty}$ tends to zero and $\{h(\theta_{z_t})\}_{t=1}^{T}$ tends to be more concentrated as a result.

HDP-HSMM is an augmentation of the HDP-HMM to include duration distributions. Continuing from the notation above yields:
\begin{itemize}
  \item $\beta_k \coloneqq \beta_k'\cdot \prod_{i=1}^{k-1} (1-\beta_i')$ where $\beta_i' \overset{\text{i.i.d}}{\sim} Beta(1,\gamma)$
  \item $\pi_k \overset{\text{i.i.d}}{\sim} DP(H(\beta), \alpha)$ for $k = 1, \dots, \infty$ where $\beta = \{\beta_k\}_{k=1}^{\infty}$
  \item $z_s \sim \tilde{\pi}_{z_{s-1}}$
  \item $D_s \sim g(\omega_{z_s})$ where $\omega_k \sim G$ for $k = 1, \dots, \infty$
  \item $z'_{t_{s}:t'_{s}} = z_s$ where $t_{s} = \sum_{s' < s} D_s + 1$ and $t'_{s} = t_{s} + D_s - 1$
  \item $x_{t_{s}: t'_{s}} \overset{\text{i.i.d}}{\sim} h(\theta_{z'_t})$ where $\theta_{k} \sim H$ for $k = 1, \dots, \infty$
\end{itemize}

The main differences here are that we introduce a base distribution $G$ for drawing durations $D_s$ in a similar way that $H$ is defined in the HSMM setting which we use for drawing $\pi$. The duration distribution sequence of sub-states $z'_{t_{s}:t'_{s}}$ is set to the corresponding super-state $z_{s}$ where $t_{s}$ is the starting index for the sub-state sequence corresponding to super-state $z_s$, and $t'_{s}$ is the end index for the sub-state sequence corresponding to super-state $z_s$. The observation sequence is then drawn from the sub-state sequence using $H$. Note that $z_s$ is drawn from $\tilde{pi}_{k} \coloneqq \frac{\pi_{k,j}}{1-\pi_{k,k}}(1 - \delta_{k,j})$, which normalizes $\pi_k$ to prevent self-transitions in the super-state sequence $z_s$. Figure ~\ref{fig:hdphsmm} extends the previous HSMM diagram to illustrate the connections between the HDP component of the model, and the HSMM.

\begin{figure}[H]
\centering
\includegraphics[scale=0.12]{images/hdphsmm.png}
\caption{HDP-HSMM incorporates a HDP component into a HSMM}
\label{fig:hdphsmm}
\end{figure}

\section{Simulating Agent States and Observations}

To generate the agent pathways and observations, we varied the following grid settings and motion model assumptions:

\begin{enumerate}
	\item \textbf{Grid Size}: We generated pathways on 5$\times$5 grids, so each grid has 25 hidden states.
	\item \textbf{Obstacles}: Obstacles are placed in each grid by making certain states impossible to visit -- this is analogous to a robot encountering physical boundaries, like walls or other objects. The number and placement of obstacles in each grid is varied with the expectation that increasing the number of obstacles will improve the accuracy of the estimation methods, as there are fewer feasible paths for the agent.
	\item \textbf{Markov Property}: We simulated three different settings for the transition matrix between states, two of which did not violate the Markov assumption and one that did. Further details of these settings can be found in Section 2.2 below.
\end{enumerate}

\subsection{GridWorld Setup}

We simulated pathways on the following three grids. Each square in the grid either has a number representing its true observation, or is colored black, indicating an obstacle.

\newcommand{\addmapa}{\includegraphics[width=10em]{data/Model1/5x5;4;free.png}}
\newcommand{\addmapb}{\includegraphics[width=10em]{data/Model2/5x5;4;6box.png}}
\newcommand{\addmapc}{\includegraphics[width=10em]{data/Model3/5x5;4;6sep.png}}
\newcolumntype{C}{>{\centering\arraybackslash}m{10em}}
\begin{table}[H]
\sffamily
\centering
\begin{tabular}{l*4{C}@{}}
& \addmapa & \addmapb & \addmapc \\
\end{tabular}
\caption{GridWorld Maps}
\end{table}

\subsection{Motion Settings}

As stated above, the path for each grid was simulated using three different settings for the transitions between states.

\begin{enumerate}
	\item \textbf{Purely Random}: The agent transitions from state to state with a uniform transition probability over all of the states (including its current state). Therefore, the state sequence is essentially purely random, with each step randomly generated with replacement from the set of available states. This setting satisfies the Markov property, as each realization of a future state is independent from the history of all previous states.
	\item \textbf{Traditional Markov}: The state sequences are generated using a fixed transition matrix -- the probability of transitioning from one state to the next is constant as the sequence is generated (with one state selected as the starting state), and each step is independent of past states conditional on the present state. This setting satisfies the Markov property, so we would expect the HMM method to do well on the data simulated from it, in both estimating the transition probability matrix and in the accuracy of the maximum likelihood state sequence.
	\item \textbf{Sticky Transitions}: The HDP-HSMM specializes in inferring observations when the underlying motion is presumed to be "sticky". Specifically, a sticky process is one where transition probabilities correspond to an agent's elapsed duration in the same state. In our implementation, the probability of staying in the same state decreases geometrically with the duration, and resets upon entering a new state.
 \end{enumerate}

To generate the "true" transition matrix from which the robot path would be simulated, we first generated a matrix where the probability of staying in the same state was 0.2. This probability was constant in the Traditional Markov setting, while it decreased geometrically in the Sticky Transitions setting. We then perturbed this probability using a randomly generated value between -0.1 and 0.1. To fill out the matrix, we evenly distributed the remaining probability among valid adjacent states.

\subsection{Observation Settings}

To compare the original HMM and HDP-HSMM, we simulate observations that are continuous measurements. Each state is assigned a true mean $\mu$, while the standard deviation is identical and fixed at $\sigma = 0.5$ across all states. There were eight possible true values of $\mu$, so some states had the same observation mean. The mean value is similar to the discrete setting, where in each state, the robot observes a Gaussian sample given the mean of the state.

To generate our data, for each of the 9 settings (3 grids x 3 transition settings), we simulated 10 state sequences of length 200. Then, we simulated observations from these state sequences using the emission distributions (the "true" observation for each state was also randomly assigned). These generated observation sequences were used as the inputs for our HMM and HDP-HSMM.

\section{Results}

To measure the accuracy of the HMM and HDP-HSMM methods, we measured the misclassification rate between the state sequences estimated by each method from the observation sequences and the true state sequences from which the observations were generated. Our results below are the median misclassification rate across our 10 generated sequences using each method.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
	\hline
	& Map 1 & Map 2 & Map 3 \\
	\hline
	Random	& 0.908 & 0.908 & 0.908  \\
	\hline
	Markov & 0.483 & 0.435 & 0.440 \\
	\hline
	Sticky & 0.460 & 0.475& 0.503 \\
	\hline
	\end{tabular}
	\caption{HMM Results}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		& Map 1 & Map 2 & Map 3 \\
		\hline
		Random	& 0.908 &  &   \\
		\hline
		Markov & 0.768 & 0.758 &  \\
		\hline
		Sticky & 0.790 & 0.788 &  \\
		\hline
	\end{tabular}
	\caption{HDP-HSMM Results}
\end{table}

\section{Discussion}

\subsection{Markov Property and Obstacles in HMM}

The results of applying the Baum-Welch and Viterbi algorithm to our simulated sequences largely confirm that relaxing the Markov assumptions does not lead to significantly worse results. In the two maps with obstacles, the misclassification rate in the sticky transition setting was 4-6 percentage points worse than the Markovian setting, and in the map without obstacles the sticky setting actually got a lower misclassification rate. This small reduction in accuracy is acceptable, as it allows the application of the method to many more data sets -- the Markov assumption is frequently violated by real data.\\
\\
An interesting result was the effect of the number and placement of obstacles on the map. In the Markovian setting, the misclassification rate for the two maps with obstacles, Maps 2 and 3, were lower than the rate for Map 1, with no obstacles. The most likely explanation is that, by reducing the total number of valid states and the number of possible pathways, obstacles actually make it easier to estimate the hidden state sequence of the agent. This pattern was reversed in the sticky transition setting, though we do not have a strong intuitive explanation for why this is the case. \\
\\
We also discovered observation generation settings under which the method performed very poorly. If we set the number of possible true $\mu$ values to be small -- so many states shared the same mean -- the misclassification rate was sharply higher. The more possibilities there were, the better the algorithm performed. This made sense to us, as the method would have a hard time determining which state a certain observation came from if many states had a similarly high likelihood under the emission distribution.



\subsection{Comparing HMM and HSMM}




\section{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PDF files}

Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''

Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.

\begin{itemize}

\item You should directly generate PDF files using \verb+pdflatex+.

\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.

\item The IEEE has recommendations for generating PDF files whose fonts are also
  acceptable for NeurIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.

\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.

\end{itemize}

If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.

\subsection{Margins in \LaTeX{}}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All acknowledgments
go at the end of the paper. Do not include acknowledgments in the anonymized
submission, only in the final paper.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references. {\bf Remember that you can use more than eight
  pages as long as the additional pages contain \emph{only} cited references.}
\medskip

\small

[1] Johnson, M.J.\ \& Willsky, A.S.\ (2013) Bayesian Nonparametric Hidden Semi-Markov Models, {\it Journal of Machine Learning Research 14 (2013)},
pp.\ 673--701. Cambridge, MA: MIT Press.

\end{document}